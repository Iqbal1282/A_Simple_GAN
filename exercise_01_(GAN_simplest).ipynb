{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_safari(folder):\n",
    "\n",
    "    mypath = os.path.join(\"./data\", folder)\n",
    "    txt_name_list = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(mypath):\n",
    "        for f in filenames:\n",
    "            if f != '.DS_Store':\n",
    "                txt_name_list.append(f)\n",
    "                break\n",
    "\n",
    "    slice_train = int(80000/len(txt_name_list))  ###Setting value to be 80000 for the final dataset\n",
    "    i = 0\n",
    "    seed = np.random.randint(1, 10e6)\n",
    "\n",
    "    for txt_name in txt_name_list:\n",
    "        txt_path = os.path.join(mypath,txt_name)\n",
    "        x = np.load(txt_path)\n",
    "        x = (x.astype('float32') - 127.5) / 127.5\n",
    "        # x = x.astype('float32') / 255.0\n",
    "        \n",
    "        x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "        \n",
    "        y = [i] * len(x)  \n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "        x = x[:slice_train]\n",
    "        y = y[:slice_train]\n",
    "        if i != 0: \n",
    "            xtotal = np.concatenate((x,xtotal), axis=0)\n",
    "            ytotal = np.concatenate((y,ytotal), axis=0)\n",
    "        else:\n",
    "            xtotal = x\n",
    "            ytotal = y\n",
    "        i += 1\n",
    "        \n",
    "    return xtotal, ytotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train) = load_safari('camel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2cbb804d898>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[200,:,:,0], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GAN import GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "discriminator_input (InputLa [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "Conv2D_layer1 (Conv2D)       (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_layer2 (Conv2D)       (None, 7, 7, 64)          102464    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_layer3 (Conv2D)       (None, 4, 4, 128)         204928    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_layer4 (Conv2D)       (None, 2, 2, 128)         409728    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 719,297\n",
      "Trainable params: 719,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3136)              316736    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "layer2_upsample (Conv2D)     (None, 14, 14, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "layer3_upsample (Conv2D)     (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "layer_4 (Conv2DTranspose)    (None, 28, 28, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "generator_conv_last (Conv2DT (None, 28, 28, 1)         1601      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 844,161\n",
      "Trainable params: 837,377\n",
      "Non-trainable params: 6,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 400\n",
    "PRINT_EVERY_N_BATCHES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: (0.709)(R 0.693, F 0.726)] [D acc: (0.289)(0.578, 0.000)] [G loss: 0.689] [G acc: 1.000]\n",
      "1 [D loss: (0.686)(R 0.664, F 0.707)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.478] [G acc: 1.000]\n",
      "2 [D loss: (10.477)(R 0.293, F 20.661)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.305] [G acc: 1.000]\n",
      "3 [D loss: (0.538)(R 0.330, F 0.746)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.244] [G acc: 1.000]\n",
      "4 [D loss: (0.595)(R 0.288, F 0.902)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.221] [G acc: 1.000]\n",
      "5 [D loss: (0.654)(R 0.285, F 1.024)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.220] [G acc: 1.000]\n",
      "6 [D loss: (0.692)(R 0.294, F 1.090)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.248] [G acc: 1.000]\n",
      "7 [D loss: (0.697)(R 0.318, F 1.077)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.280] [G acc: 1.000]\n",
      "8 [D loss: (0.708)(R 0.336, F 1.080)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.328] [G acc: 1.000]\n",
      "9 [D loss: (0.703)(R 0.366, F 1.039)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.361] [G acc: 1.000]\n",
      "10 [D loss: (0.694)(R 0.386, F 1.001)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.400] [G acc: 1.000]\n",
      "11 [D loss: (0.692)(R 0.400, F 0.984)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.441] [G acc: 1.000]\n",
      "12 [D loss: (0.682)(R 0.415, F 0.949)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.462] [G acc: 1.000]\n",
      "13 [D loss: (0.685)(R 0.426, F 0.945)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.504] [G acc: 1.000]\n",
      "14 [D loss: (0.672)(R 0.426, F 0.919)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.529] [G acc: 1.000]\n",
      "15 [D loss: (0.672)(R 0.428, F 0.917)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.574] [G acc: 1.000]\n",
      "16 [D loss: (0.647)(R 0.411, F 0.884)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.607] [G acc: 1.000]\n",
      "17 [D loss: (0.620)(R 0.396, F 0.845)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.636] [G acc: 1.000]\n",
      "18 [D loss: (0.568)(R 0.331, F 0.805)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.665] [G acc: 1.000]\n",
      "19 [D loss: (0.512)(R 0.256, F 0.767)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.682] [G acc: 1.000]\n",
      "20 [D loss: (0.429)(R 0.135, F 0.722)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.693] [G acc: 0.469]\n",
      "21 [D loss: (0.386)(R 0.073, F 0.698)] [D acc: (0.625)(1.000, 0.250)] [G loss: 0.705] [G acc: 0.016]\n",
      "22 [D loss: (0.360)(R 0.034, F 0.685)] [D acc: (0.922)(1.000, 0.844)] [G loss: 0.730] [G acc: 0.000]\n",
      "23 [D loss: (0.342)(R 0.022, F 0.663)] [D acc: (0.992)(1.000, 0.984)] [G loss: 0.811] [G acc: 0.000]\n",
      "24 [D loss: (0.307)(R 0.018, F 0.595)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.074] [G acc: 0.000]\n",
      "25 [D loss: (0.221)(R 0.008, F 0.434)] [D acc: (1.000)(1.000, 1.000)] [G loss: 1.632] [G acc: 0.000]\n",
      "26 [D loss: (0.641)(R 0.041, F 1.241)] [D acc: (0.516)(0.984, 0.047)] [G loss: 1.718] [G acc: 0.000]\n",
      "27 [D loss: (1.535)(R 2.688, F 0.382)] [D acc: (0.500)(0.000, 1.000)] [G loss: 0.019] [G acc: 0.984]\n",
      "28 [D loss: (0.200)(R 0.061, F 0.339)] [D acc: (1.000)(1.000, 1.000)] [G loss: 0.002] [G acc: 1.000]\n",
      "29 [D loss: (0.219)(R 0.049, F 0.389)] [D acc: (0.984)(1.000, 0.969)] [G loss: 0.001] [G acc: 1.000]\n",
      "30 [D loss: (0.342)(R 0.088, F 0.596)] [D acc: (0.820)(0.984, 0.656)] [G loss: 0.002] [G acc: 1.000]\n",
      "31 [D loss: (0.502)(R 0.092, F 0.912)] [D acc: (0.734)(1.000, 0.469)] [G loss: 0.032] [G acc: 0.984]\n",
      "32 [D loss: (0.441)(R 0.603, F 0.279)] [D acc: (0.820)(0.672, 0.969)] [G loss: 0.000] [G acc: 1.000]\n",
      "33 [D loss: (0.238)(R 0.066, F 0.410)] [D acc: (0.922)(1.000, 0.844)] [G loss: 0.001] [G acc: 1.000]\n",
      "34 [D loss: (0.432)(R 0.129, F 0.736)] [D acc: (0.805)(0.953, 0.656)] [G loss: 0.010] [G acc: 1.000]\n",
      "35 [D loss: (1.101)(R 0.429, F 1.774)] [D acc: (0.531)(0.797, 0.266)] [G loss: 0.163] [G acc: 1.000]\n",
      "36 [D loss: (1.543)(R 0.641, F 2.446)] [D acc: (0.328)(0.656, 0.000)] [G loss: 0.631] [G acc: 0.859]\n",
      "37 [D loss: (0.746)(R 0.599, F 0.893)] [D acc: (0.445)(0.891, 0.000)] [G loss: 0.637] [G acc: 0.891]\n",
      "38 [D loss: (0.718)(R 0.603, F 0.832)] [D acc: (0.445)(0.891, 0.000)] [G loss: 0.679] [G acc: 0.641]\n",
      "39 [D loss: (0.703)(R 0.590, F 0.815)] [D acc: (0.469)(0.938, 0.000)] [G loss: 0.678] [G acc: 0.750]\n",
      "40 [D loss: (0.681)(R 0.560, F 0.802)] [D acc: (0.477)(0.953, 0.000)] [G loss: 0.680] [G acc: 0.688]\n",
      "41 [D loss: (0.670)(R 0.535, F 0.804)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.685] [G acc: 0.672]\n",
      "42 [D loss: (0.659)(R 0.510, F 0.807)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.696] [G acc: 0.453]\n",
      "43 [D loss: (0.669)(R 0.493, F 0.844)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.698] [G acc: 0.297]\n",
      "44 [D loss: (0.674)(R 0.467, F 0.880)] [D acc: (0.500)(1.000, 0.000)] [G loss: 0.710] [G acc: 0.094]\n",
      "45 [D loss: (0.615)(R 0.470, F 0.761)] [D acc: (0.516)(1.000, 0.031)] [G loss: 0.732] [G acc: 0.000]\n",
      "46 [D loss: (0.598)(R 0.425, F 0.771)] [D acc: (0.523)(1.000, 0.047)] [G loss: 0.750] [G acc: 0.000]\n",
      "47 [D loss: (0.582)(R 0.430, F 0.734)] [D acc: (0.648)(1.000, 0.297)] [G loss: 0.769] [G acc: 0.031]\n",
      "48 [D loss: (0.546)(R 0.336, F 0.755)] [D acc: (0.672)(1.000, 0.344)] [G loss: 0.818] [G acc: 0.000]\n",
      "49 [D loss: (0.564)(R 0.352, F 0.776)] [D acc: (0.641)(0.969, 0.312)] [G loss: 0.787] [G acc: 0.109]\n",
      "50 [D loss: (0.920)(R 0.287, F 1.554)] [D acc: (0.500)(0.984, 0.016)] [G loss: 0.900] [G acc: 0.125]\n",
      "51 [D loss: (0.677)(R 0.570, F 0.784)] [D acc: (0.383)(0.750, 0.016)] [G loss: 0.698] [G acc: 0.281]\n",
      "52 [D loss: (0.572)(R 0.423, F 0.721)] [D acc: (0.594)(0.969, 0.219)] [G loss: 0.705] [G acc: 0.250]\n",
      "53 [D loss: (0.505)(R 0.300, F 0.711)] [D acc: (0.789)(1.000, 0.578)] [G loss: 0.737] [G acc: 0.188]\n",
      "54 [D loss: (0.584)(R 0.214, F 0.954)] [D acc: (0.625)(1.000, 0.250)] [G loss: 0.858] [G acc: 0.078]\n",
      "55 [D loss: (0.850)(R 0.357, F 1.342)] [D acc: (0.516)(1.000, 0.031)] [G loss: 0.727] [G acc: 0.188]\n",
      "56 [D loss: (0.618)(R 0.502, F 0.734)] [D acc: (0.656)(0.969, 0.344)] [G loss: 0.873] [G acc: 0.047]\n",
      "57 [D loss: (0.640)(R 0.540, F 0.740)] [D acc: (0.625)(0.891, 0.359)] [G loss: 0.804] [G acc: 0.016]\n",
      "58 [D loss: (0.645)(R 0.446, F 0.843)] [D acc: (0.547)(0.969, 0.125)] [G loss: 0.754] [G acc: 0.031]\n",
      "59 [D loss: (0.662)(R 0.457, F 0.867)] [D acc: (0.492)(0.953, 0.031)] [G loss: 0.728] [G acc: 0.219]\n",
      "60 [D loss: (0.627)(R 0.448, F 0.806)] [D acc: (0.523)(0.984, 0.062)] [G loss: 0.754] [G acc: 0.062]\n",
      "61 [D loss: (0.630)(R 0.458, F 0.801)] [D acc: (0.500)(0.969, 0.031)] [G loss: 0.732] [G acc: 0.094]\n",
      "62 [D loss: (0.598)(R 0.415, F 0.782)] [D acc: (0.523)(0.969, 0.078)] [G loss: 0.807] [G acc: 0.016]\n",
      "63 [D loss: (0.552)(R 0.411, F 0.694)] [D acc: (0.812)(0.969, 0.656)] [G loss: 0.901] [G acc: 0.016]\n",
      "64 [D loss: (0.546)(R 0.337, F 0.755)] [D acc: (0.672)(1.000, 0.344)] [G loss: 0.962] [G acc: 0.000]\n",
      "65 [D loss: (0.579)(R 0.321, F 0.836)] [D acc: (0.602)(0.969, 0.234)] [G loss: 1.024] [G acc: 0.000]\n",
      "66 [D loss: (0.521)(R 0.407, F 0.636)] [D acc: (0.891)(0.906, 0.875)] [G loss: 0.932] [G acc: 0.000]\n",
      "67 [D loss: (0.491)(R 0.219, F 0.764)] [D acc: (0.695)(0.984, 0.406)] [G loss: 1.139] [G acc: 0.000]\n",
      "68 [D loss: (0.681)(R 0.654, F 0.707)] [D acc: (0.648)(0.703, 0.594)] [G loss: 0.737] [G acc: 0.250]\n",
      "69 [D loss: (0.388)(R 0.216, F 0.560)] [D acc: (0.883)(0.969, 0.797)] [G loss: 0.667] [G acc: 0.516]\n",
      "70 [D loss: (0.851)(R 0.208, F 1.494)] [D acc: (0.523)(0.938, 0.109)] [G loss: 0.849] [G acc: 0.062]\n",
      "71 [D loss: (0.684)(R 0.772, F 0.595)] [D acc: (0.641)(0.469, 0.812)] [G loss: 0.677] [G acc: 0.484]\n",
      "72 [D loss: (0.387)(R 0.406, F 0.368)] [D acc: (0.945)(0.922, 0.969)] [G loss: 0.357] [G acc: 1.000]\n",
      "73 [D loss: (0.700)(R 0.290, F 1.110)] [D acc: (0.641)(0.938, 0.344)] [G loss: 0.603] [G acc: 0.750]\n",
      "74 [D loss: (0.905)(R 0.541, F 1.269)] [D acc: (0.383)(0.750, 0.016)] [G loss: 0.838] [G acc: 0.000]\n",
      "75 [D loss: (0.533)(R 0.420, F 0.647)] [D acc: (0.875)(0.953, 0.797)] [G loss: 0.841] [G acc: 0.000]\n",
      "76 [D loss: (0.396)(R 0.274, F 0.517)] [D acc: (0.891)(0.953, 0.828)] [G loss: 0.820] [G acc: 0.141]\n",
      "77 [D loss: (0.564)(R 0.339, F 0.789)] [D acc: (0.547)(0.859, 0.234)] [G loss: 1.005] [G acc: 0.000]\n",
      "78 [D loss: (0.595)(R 0.257, F 0.934)] [D acc: (0.531)(0.922, 0.141)] [G loss: 0.978] [G acc: 0.000]\n",
      "79 [D loss: (0.666)(R 0.414, F 0.917)] [D acc: (0.508)(0.812, 0.203)] [G loss: 0.787] [G acc: 0.016]\n",
      "80 [D loss: (0.629)(R 0.425, F 0.834)] [D acc: (0.484)(0.875, 0.094)] [G loss: 0.756] [G acc: 0.266]\n",
      "81 [D loss: (0.508)(R 0.332, F 0.684)] [D acc: (0.789)(0.906, 0.672)] [G loss: 0.733] [G acc: 0.312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 [D loss: (0.719)(R 0.327, F 1.111)] [D acc: (0.484)(0.969, 0.000)] [G loss: 0.676] [G acc: 0.562]\n",
      "83 [D loss: (0.603)(R 0.288, F 0.918)] [D acc: (0.531)(0.969, 0.094)] [G loss: 0.754] [G acc: 0.141]\n",
      "84 [D loss: (0.576)(R 0.415, F 0.738)] [D acc: (0.656)(0.953, 0.359)] [G loss: 0.718] [G acc: 0.219]\n",
      "85 [D loss: (0.580)(R 0.294, F 0.866)] [D acc: (0.578)(0.953, 0.203)] [G loss: 0.780] [G acc: 0.031]\n",
      "86 [D loss: (0.555)(R 0.373, F 0.737)] [D acc: (0.664)(0.938, 0.391)] [G loss: 0.782] [G acc: 0.094]\n",
      "87 [D loss: (0.570)(R 0.287, F 0.853)] [D acc: (0.562)(0.953, 0.172)] [G loss: 0.847] [G acc: 0.016]\n",
      "88 [D loss: (0.522)(R 0.400, F 0.643)] [D acc: (0.875)(0.906, 0.844)] [G loss: 0.832] [G acc: 0.078]\n",
      "89 [D loss: (0.495)(R 0.214, F 0.776)] [D acc: (0.664)(0.969, 0.359)] [G loss: 0.964] [G acc: 0.000]\n",
      "90 [D loss: (0.526)(R 0.423, F 0.628)] [D acc: (0.820)(0.781, 0.859)] [G loss: 0.858] [G acc: 0.094]\n",
      "91 [D loss: (0.482)(R 0.128, F 0.836)] [D acc: (0.680)(1.000, 0.359)] [G loss: 1.140] [G acc: 0.000]\n",
      "92 [D loss: (0.604)(R 0.605, F 0.602)] [D acc: (0.758)(0.625, 0.891)] [G loss: 0.971] [G acc: 0.016]\n",
      "93 [D loss: (0.598)(R 0.165, F 1.030)] [D acc: (0.633)(1.000, 0.266)] [G loss: 1.095] [G acc: 0.000]\n",
      "94 [D loss: (0.640)(R 0.686, F 0.594)] [D acc: (0.734)(0.516, 0.953)] [G loss: 0.884] [G acc: 0.016]\n",
      "95 [D loss: (0.534)(R 0.334, F 0.735)] [D acc: (0.719)(0.938, 0.500)] [G loss: 0.895] [G acc: 0.000]\n",
      "96 [D loss: (0.526)(R 0.346, F 0.706)] [D acc: (0.742)(0.875, 0.609)] [G loss: 0.891] [G acc: 0.000]\n",
      "97 [D loss: (0.906)(R 0.340, F 1.471)] [D acc: (0.461)(0.906, 0.016)] [G loss: 0.902] [G acc: 0.000]\n",
      "98 [D loss: (0.534)(R 0.520, F 0.548)] [D acc: (0.805)(0.719, 0.891)] [G loss: 0.870] [G acc: 0.094]\n",
      "99 [D loss: (0.535)(R 0.343, F 0.726)] [D acc: (0.695)(0.875, 0.516)] [G loss: 1.178] [G acc: 0.000]\n",
      "100 [D loss: (0.463)(R 0.292, F 0.634)] [D acc: (0.859)(0.938, 0.781)] [G loss: 0.944] [G acc: 0.016]\n",
      "101 [D loss: (0.699)(R 0.189, F 1.209)] [D acc: (0.516)(0.969, 0.062)] [G loss: 1.099] [G acc: 0.000]\n",
      "102 [D loss: (0.633)(R 0.546, F 0.720)] [D acc: (0.633)(0.688, 0.578)] [G loss: 0.910] [G acc: 0.016]\n",
      "103 [D loss: (0.626)(R 0.270, F 0.983)] [D acc: (0.555)(0.953, 0.156)] [G loss: 0.980] [G acc: 0.000]\n",
      "104 [D loss: (0.541)(R 0.486, F 0.596)] [D acc: (0.930)(0.859, 1.000)] [G loss: 0.871] [G acc: 0.062]\n",
      "105 [D loss: (0.478)(R 0.212, F 0.743)] [D acc: (0.727)(0.984, 0.469)] [G loss: 1.032] [G acc: 0.031]\n",
      "106 [D loss: (0.563)(R 0.326, F 0.801)] [D acc: (0.633)(0.875, 0.391)] [G loss: 1.159] [G acc: 0.000]\n",
      "107 [D loss: (0.513)(R 0.398, F 0.629)] [D acc: (0.789)(0.859, 0.719)] [G loss: 1.142] [G acc: 0.000]\n",
      "108 [D loss: (0.535)(R 0.293, F 0.776)] [D acc: (0.711)(0.922, 0.500)] [G loss: 1.186] [G acc: 0.000]\n",
      "109 [D loss: (0.515)(R 0.361, F 0.670)] [D acc: (0.812)(0.844, 0.781)] [G loss: 1.063] [G acc: 0.000]\n",
      "110 [D loss: (0.608)(R 0.281, F 0.934)] [D acc: (0.586)(0.859, 0.312)] [G loss: 1.136] [G acc: 0.000]\n",
      "111 [D loss: (0.558)(R 0.507, F 0.610)] [D acc: (0.781)(0.734, 0.828)] [G loss: 0.962] [G acc: 0.172]\n",
      "112 [D loss: (0.330)(R 0.153, F 0.507)] [D acc: (0.953)(0.984, 0.922)] [G loss: 0.952] [G acc: 0.156]\n",
      "113 [D loss: (1.052)(R 0.114, F 1.991)] [D acc: (0.500)(0.969, 0.031)] [G loss: 1.368] [G acc: 0.000]\n",
      "114 [D loss: (0.781)(R 1.000, F 0.562)] [D acc: (0.531)(0.094, 0.969)] [G loss: 0.951] [G acc: 0.000]\n",
      "115 [D loss: (0.568)(R 0.441, F 0.694)] [D acc: (0.742)(0.812, 0.672)] [G loss: 0.905] [G acc: 0.016]\n",
      "116 [D loss: (0.768)(R 0.361, F 1.174)] [D acc: (0.578)(0.922, 0.234)] [G loss: 0.894] [G acc: 0.016]\n",
      "117 [D loss: (0.734)(R 0.590, F 0.879)] [D acc: (0.461)(0.672, 0.250)] [G loss: 0.822] [G acc: 0.125]\n",
      "118 [D loss: (0.756)(R 0.616, F 0.896)] [D acc: (0.445)(0.672, 0.219)] [G loss: 0.826] [G acc: 0.062]\n",
      "119 [D loss: (0.713)(R 0.652, F 0.773)] [D acc: (0.484)(0.562, 0.406)] [G loss: 0.796] [G acc: 0.094]\n",
      "120 [D loss: (0.692)(R 0.632, F 0.752)] [D acc: (0.555)(0.703, 0.406)] [G loss: 0.774] [G acc: 0.109]\n",
      "121 [D loss: (0.720)(R 0.595, F 0.846)] [D acc: (0.484)(0.734, 0.234)] [G loss: 0.795] [G acc: 0.109]\n",
      "122 [D loss: (0.656)(R 0.641, F 0.672)] [D acc: (0.648)(0.672, 0.625)] [G loss: 0.839] [G acc: 0.078]\n",
      "123 [D loss: (0.809)(R 0.637, F 0.981)] [D acc: (0.406)(0.656, 0.156)] [G loss: 0.928] [G acc: 0.016]\n",
      "124 [D loss: (0.685)(R 0.712, F 0.659)] [D acc: (0.562)(0.406, 0.719)] [G loss: 0.798] [G acc: 0.047]\n",
      "125 [D loss: (0.591)(R 0.657, F 0.525)] [D acc: (0.719)(0.625, 0.812)] [G loss: 1.003] [G acc: 0.047]\n",
      "126 [D loss: (0.729)(R 0.918, F 0.540)] [D acc: (0.656)(0.359, 0.953)] [G loss: 1.421] [G acc: 0.000]\n",
      "127 [D loss: (0.704)(R 0.616, F 0.792)] [D acc: (0.547)(0.656, 0.438)] [G loss: 0.855] [G acc: 0.016]\n",
      "128 [D loss: (0.670)(R 0.566, F 0.774)] [D acc: (0.578)(0.828, 0.328)] [G loss: 0.758] [G acc: 0.141]\n",
      "129 [D loss: (0.658)(R 0.602, F 0.714)] [D acc: (0.672)(0.734, 0.609)] [G loss: 0.768] [G acc: 0.172]\n",
      "130 [D loss: (0.689)(R 0.551, F 0.828)] [D acc: (0.578)(0.938, 0.219)] [G loss: 0.744] [G acc: 0.172]\n",
      "131 [D loss: (0.677)(R 0.591, F 0.762)] [D acc: (0.547)(0.766, 0.328)] [G loss: 0.725] [G acc: 0.266]\n",
      "132 [D loss: (0.700)(R 0.598, F 0.803)] [D acc: (0.531)(0.875, 0.188)] [G loss: 0.710] [G acc: 0.312]\n",
      "133 [D loss: (0.698)(R 0.612, F 0.783)] [D acc: (0.500)(0.797, 0.203)] [G loss: 0.687] [G acc: 0.531]\n",
      "134 [D loss: (0.697)(R 0.633, F 0.761)] [D acc: (0.453)(0.688, 0.219)] [G loss: 0.684] [G acc: 0.562]\n",
      "135 [D loss: (0.721)(R 0.598, F 0.844)] [D acc: (0.438)(0.781, 0.094)] [G loss: 0.756] [G acc: 0.109]\n",
      "136 [D loss: (0.678)(R 0.661, F 0.695)] [D acc: (0.617)(0.719, 0.516)] [G loss: 0.742] [G acc: 0.203]\n",
      "137 [D loss: (0.684)(R 0.604, F 0.764)] [D acc: (0.602)(0.859, 0.344)] [G loss: 0.768] [G acc: 0.141]\n",
      "138 [D loss: (0.714)(R 0.639, F 0.789)] [D acc: (0.469)(0.734, 0.203)] [G loss: 0.826] [G acc: 0.047]\n",
      "139 [D loss: (0.635)(R 0.661, F 0.609)] [D acc: (0.742)(0.672, 0.812)] [G loss: 0.911] [G acc: 0.031]\n",
      "140 [D loss: (0.779)(R 0.830, F 0.729)] [D acc: (0.383)(0.297, 0.469)] [G loss: 1.197] [G acc: 0.000]\n",
      "141 [D loss: (0.791)(R 0.889, F 0.693)] [D acc: (0.359)(0.109, 0.609)] [G loss: 0.735] [G acc: 0.062]\n",
      "142 [D loss: (0.648)(R 0.665, F 0.631)] [D acc: (0.656)(0.672, 0.641)] [G loss: 0.738] [G acc: 0.156]\n",
      "143 [D loss: (0.688)(R 0.647, F 0.729)] [D acc: (0.578)(0.766, 0.391)] [G loss: 0.741] [G acc: 0.094]\n",
      "144 [D loss: (0.651)(R 0.614, F 0.688)] [D acc: (0.672)(0.781, 0.562)] [G loss: 0.757] [G acc: 0.188]\n",
      "145 [D loss: (0.694)(R 0.648, F 0.740)] [D acc: (0.531)(0.750, 0.312)] [G loss: 0.742] [G acc: 0.109]\n"
     ]
    }
   ],
   "source": [
    "gan.train(     \n",
    "    x_train\n",
    "    , batch_size = BATCH_SIZE\n",
    "    , epochs = EPOCHS\n",
    "    , run_folder = 'history/GAN/Camel_dataset'\n",
    "    , print_every_n_batches = PRINT_EVERY_N_BATCHES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self,\n",
    "                input_dim, \n",
    "                discriminator_conv_filters, \n",
    "                discriminator_conv_kernel_size, \n",
    "                discriminator_conv_strides, \n",
    "                discriminator_batch_norm_momentum, \n",
    "                discriminator_activation, \n",
    "                discriminator_dropout_rate, \n",
    "                discriminator_learning_rate, \n",
    "                generator_initial_dense_layer_size, \n",
    "                generator_upsample, \n",
    "                generator_conv_filters, \n",
    "                generator_conv_kernel_size, \n",
    "                generator_conv_strides, \n",
    "                generator_batch_norm_momentum, \n",
    "                generator_activation, \n",
    "                generator_dropout_rate, \n",
    "                generator_learning_rate,\n",
    "                optimiser,\n",
    "                z_dim,\n",
    "                 \n",
    "                ):\n",
    "        self.name = 'gan'\n",
    "        self.input_dim = input_dim \n",
    "        self.discriminator_conv_filters = discriminator_conv_filters\n",
    "        self.discriminator_conv_kernel_size = disciriminator_conv_kernel_size\n",
    "        self.discriminator_conv_strides = discriminator_conv_strides \n",
    "        self.discriminator_batch_norm_monentum = discriminator_batch_norm_monentum\n",
    "        self.discriminator_activation = discriminator_activation \n",
    "        self.discriminator_dropout_rate = discriminator_dropout_rate\n",
    "        self.discriminator_learning_rate = discriminator_learning_rate\n",
    "        \n",
    "        \n",
    "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
    "        self.generator_upsample = generator_upsample \n",
    "        self.generator_conv_filters = generator_conv_filters \n",
    "        self.generator_conv_kernel_size = generator_conv_kernel_size \n",
    "        self.generator_conv_strides = generator_conv_strides \n",
    "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
    "        self.generator_activation = generator_activation \n",
    "        self.generator_dropout_rate = generator_dropout_rate \n",
    "        self.generator_learning_rate = generator_learning_rate \n",
    "        \n",
    "        self.optimiser = optimiser \n",
    "        self.z_dim = z_dim \n",
    "        \n",
    "        self.n_layers_discriminator = len(discriminator_conv_filters)\n",
    "        self.n_layers_generator = len(generator_conv_filters)\n",
    "        \n",
    "        self.weight_init = RandomNormal(mean = 0., stddev = 0.02)\n",
    "        \n",
    "        self.d_losses = []\n",
    "        self.g_losses = []\n",
    "        self.epoch = 0 \n",
    "        \n",
    "        self._build_discirminator()\n",
    "        self._build_generator()\n",
    "        \n",
    "        self._build_adversarial()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def get_activation(self, activation):\n",
    "        if activation == 'leaky_relu'\n",
    "            layer = LeakyReLU(alpha = 0.2)\n",
    "        else: \n",
    "            layer = Activation(activation)\n",
    "        return layer \n",
    "    \n",
    "    def _build_discriminator(self):\n",
    "        ## Discriminator \n",
    "        \n",
    "        discriminator_input = Input(shape = self.input_dim, name= 'discriminator_input')\n",
    "        \n",
    "        x = discriminator_input \n",
    "        \n",
    "        for i in range(self.n_layers_discriminator):\n",
    "            x = Conv2D(\n",
    "                    filters = self.discriminator_conv_filters[i], \n",
    "                kernel_size = self.discriminator_conv_kernel_size[i],\n",
    "                strides = self.discriminator_conv_strides[i],\n",
    "                padding = 'same',\n",
    "                name = 'discriminator_conv_'+str(i),\n",
    "                kernel_initializer = self.weight_init\n",
    "            )(x)\n",
    "            \n",
    "            \n",
    "            if self.discriminator_batch_norm_momentum and i > 0: \n",
    "                x = BatchNormalization(momentum = self.discriminator_batch_norm_momentum)(x)\n",
    "            \n",
    "            \n",
    "        \n",
    "            x = self.get_activation(self.discriminator_activation)(x)\n",
    "            if self.discriminator_dropout_rate: \n",
    "                x = Dropout(rate = self.discriminator_dropout_rate)(x)\n",
    "                \n",
    "        x =  Flatten()(x)\n",
    "        \n",
    "        discriminator_output = Dense(1, activation = 'sigmoid', kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        self.discriminator = Model(discriminator_input, discriminator_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _build_generator(self):\n",
    "        generator_input = Input(shape = (self.z_dim,), name = 'generator_input')\n",
    "        x = generator_input\n",
    "        x = Dense(np.prod(self.generator_initial_dense_layer_size), kernel_initializer = self.weight_init)(x)\n",
    "        \n",
    "        if self.generator_batch_norm_momentum: \n",
    "            x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "        x = self.get_activation(self.generator_activation)(x)\n",
    "        \n",
    "        x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
    "        \n",
    "        if self.generator_dropout_rate: \n",
    "            x = Dropout(rate = self.generator_dropout_rate)(x)\n",
    "            \n",
    "        for i in range(self.n_layers_generator):\n",
    "            if self.gnerator_upsample[i] == 2: \n",
    "                x = UpSampling2D()(x)\n",
    "                x = Conv2D(\n",
    "                    filters = self.generator_conv_filters[i],\n",
    "                    kernel_size = self.generator_conv_kernel_size[i],\n",
    "                    padding = 'same',\n",
    "                    name = 'generator_conv_' + str(i)\n",
    "                    kernel_initializer = self.weight_init\n",
    "                )(x)\n",
    "                \n",
    "            else: \n",
    "                x = Conv2DTranspose(\n",
    "                    filters = self.generator_conv_filters[i],\n",
    "                    kernel_size = self.generator_conv_kernel_size[i],\n",
    "                    padding = 'same',\n",
    "                    strides = self.generator_conv_strides[i],\n",
    "                    name = 'generator_conv' + str(i),\n",
    "                    kernel_initializer =self.weight_init\n",
    "                    \n",
    "                )(x)\n",
    "                \n",
    "                \n",
    "            if i < self.n_layers_generator -1: \n",
    "                if self.generator_batch_norm_momentum: \n",
    "                    x = BatchNormalization(momentum = self.generator_batch_norm_momentum)(x)\n",
    "                    \n",
    "                x = self.get_activation(self.generator_activation)(x)\n",
    "                \n",
    "            else: \n",
    "                x = Activation('tanh')(x)\n",
    "                \n",
    "                \n",
    "        generator_output = x \n",
    "        \n",
    "        self.generator = Model(generator_input, generator_output)\n",
    "        \n",
    "        \n",
    "    def get_opti(self, lr):\n",
    "        if self.optimizer == 'adam':\n",
    "            opti = Adam(lr = lr, beta_1 = 0.5)\n",
    "            \n",
    "        elif self.optimizer = 'rmsprop':\n",
    "            opti = RMSprop(lr = lr)\n",
    "            \n",
    "        else: \n",
    "            opti = Adam(lr= lr)\n",
    "            \n",
    "    def set_trainable(self, m, val):\n",
    "        m.trainable = val \n",
    "        for l in m.layers: \n",
    "            l.trainable = val \n",
    "            \n",
    "            \n",
    "    def _build_adversarial(self):\n",
    "        \n",
    "        ## Compile discriminator \n",
    "        \n",
    "        self.discriminator.compile(\n",
    "            optimizer = self.get_opti(self.discriminator_learning_rate),\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.set_trainable(self.discriminator, False)\n",
    "        \n",
    "        model_input = Input(shape = (self.z_dim,), name = 'model_input')\n",
    "        model_output = self.discriminator(self.generator(model_input))\n",
    "        \n",
    "        self.model = Model(model_input, model_output)\n",
    "        self.model.compile(optimizer = self.get_opti(self.generator_learning_rate), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "        \n",
    "        self.set_trainable(self.discriminator, True)\n",
    "        \n",
    "        \n",
    "    def train_discriminator(self, x_train, batch_size, using_generator):\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        if using_generator: \n",
    "            true_imgs = next(x_train)[0]\n",
    "            if true_ims.shape[0] != batch_size: \n",
    "                true_imgs = next(x_train)[0]\n",
    "                \n",
    "        else: \n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            true_imgs = x_train[idx]\n",
    "            \n",
    "        noise = np.random.normal(0,1, (batch_size, self.z_dim))\n",
    "        \n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        \n",
    "        d_loss_real, d_acc_real = self.discriminator.train_on_batch(true_imgs, valid)\n",
    "        d_loss_fake, d_acc_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "        \n",
    "        d_loss = 0.5*(d_loss_real + d_loss_fake)\n",
    "        d_acc = 0.5 * (d_acc_real + d_acc_fake)\n",
    "        \n",
    "        return [d_loss, d_loss_real, d_loss_fake, d_acc, d_acc_real, d_acc_fake]\n",
    "    \n",
    "    \n",
    "    def train_generator(self, batch_size):\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        noise = np.random.normal(0,1, (batch_size, self.z_dim))\n",
    "        \n",
    "        return self.model.train_on_batch(noise, valid)\n",
    "    \n",
    "    def train(self, x_train, batch_size, epochs, run_folder, print_every_n_batches = 50, using_generator = False):\n",
    "        \n",
    "        \n",
    "        for epoch in range(self.epoch, self.epoch+ epochs):\n",
    "            d = self.train_discriminator(x_train, batch_size, using_generator)\n",
    "            g = self.train_generator(batch_size)\n",
    "            \n",
    "            \n",
    "            print(\"%d [D loss: (%.3f)(R %.3f, F %.3f)] [D acc: (%.3f)(%.3f, %.3f)] [G loss: %.3f] [G acc: %.3f]\" % (epoch, d[0], d[1], d[2], d[3], d[4], d[5], g[0], g[1]))\n",
    "            \n",
    "            self.d_losses.append(d)\n",
    "            self.g_losses.append(g)\n",
    "            \n",
    "            if epoch % print_every_n_batches ==0:\n",
    "                self.sample_images(run_folder)\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights-%d.h5' % (epoch)))\n",
    "                self.model.save_weights(os.path.join(run_folder, 'weights/weights.h5'))\n",
    "                \n",
    "                self.save_model(run_folder)\n",
    "                \n",
    "            self.epoch +=1 \n",
    "            \n",
    "            \n",
    "    def sample_images(self, run_folder):\n",
    "        r, c = 5,5 \n",
    "        noise = np.random.normal(0, 1, (r*c, self.z_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        \n",
    "        gen_imgs = 0.5 * (gen_imgs +1)\n",
    "        gen_imgs = np.clip(gen_imgs, 0 ,1)\n",
    "        \n",
    "        fig, axs = plt.subplots(r,c, figsize = (15, 15))\n",
    "        cnt  = 0 \n",
    "        \n",
    "        for i in range(r):\n",
    "            forr j in range(c):\n",
    "                axs[i,j].imshow(np.squeeze(gen_imgs[cnt, :, :,:]), cmap = 'gray')\n",
    "                \n",
    "                axs[i, j].axis('off')\n",
    "                cnt +=1 \n",
    "                \n",
    "        for savefig(os.path.join(run_folder, 'images/sample_%d.png'%self.epoch))\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    def plot_model(self, run_folder):\n",
    "        plot_model(self.model, to_file=os.path.join(run_folder ,'viz/model.png'), show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.discriminator, to_file=os.path.join(run_folder ,'viz/discriminator.png'), show_shapes = True, show_layer_names = True)\n",
    "        plot_model(self.generator, to_file=os.path.join(run_folder ,'viz/generator.png'), show_shapes = True, show_layer_names = True)\n",
    "\n",
    "    def save(self, folder):\n",
    "\n",
    "        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:\n",
    "            pkl.dump([\n",
    "                self.input_dim\n",
    "                , self.discriminator_conv_filters\n",
    "                , self.discriminator_conv_kernel_size\n",
    "                , self.discriminator_conv_strides\n",
    "                , self.discriminator_batch_norm_momentum\n",
    "                , self.discriminator_activation\n",
    "                , self.discriminator_dropout_rate\n",
    "                , self.discriminator_learning_rate\n",
    "                , self.generator_initial_dense_layer_size\n",
    "                , self.generator_upsample\n",
    "                , self.generator_conv_filters\n",
    "                , self.generator_conv_kernel_size\n",
    "                , self.generator_conv_strides\n",
    "                , self.generator_batch_norm_momentum\n",
    "                , self.generator_activation\n",
    "                , self.generator_dropout_rate\n",
    "                , self.generator_learning_rate\n",
    "                , self.optimiser\n",
    "                , self.z_dim\n",
    "                ], f)\n",
    "\n",
    "        self.plot_model(folder)\n",
    "        \n",
    "    def save_model(self, run_folder):\n",
    "        self.model.save(os.path.join(run_folder, 'model.h5'))\n",
    "        self.discriminator.save(os.path.join(run_folder, 'discriminator.h5'))\n",
    "        self.generator.save(os.path.join(run_folder, 'generator.h5'))\n",
    "        pkl.dump(self, open( os.path.join(run_folder, \"obj.pkl\"), \"wb\" ))\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
